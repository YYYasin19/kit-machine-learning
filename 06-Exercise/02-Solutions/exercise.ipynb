{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Neural Network Classifier from Scratch (10p.)\n",
    "\n",
    "In this exercise we will implement a small neural network from scratch, i.e., only using numpy. This is nothing you would do \"in real life\" but it is a good exercise to deepen understanding. \n",
    "\n",
    "The network will consist of an arbitrary number of hidden layers with ReLU activation, a sigmoid output layer (as we are doing binary classification) and we will train it using the binary cross entropy (negative bernoulli likelihood). Ok, so lets start by importing and loading what we need. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Load our two moons (I promise we will get a new dataset in the next exercise)\n",
    "train_data = dict(np.load(\"two_moons.npz\", allow_pickle=True)) \n",
    "test_data = dict(np.load(\"two_moons_test.npz\", allow_pickle=True))\n",
    "# we need to reshape our labels so that they are [N, 1] and not [N] anymore\n",
    "train_samples, train_labels = train_data[\"samples\"], train_data[\"labels\"][:, None]\n",
    "test_samples, test_labels = test_data[\"samples\"], test_data[\"labels\"][:, None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1.1.) Auxillary Functions (3 p.)\n",
    "We start with implementing some auxillary functions we are going to need later. The sigmoid and relu activation functions, the binary cross entropy loss as well as their derviatives. \n",
    "\n",
    "The binary cross entropy loss is given as \n",
    "$ - \\dfrac{1}{N} \\sum_{i=1}^N (y_i \\log (p_i) + (1 - y_i) \\log (1 - p_i)) $ where $y_i$ denotes the ground truth label and $p_i$ the network prediction for sample $i$.\n",
    "\n",
    "**Hint** all derivatives where derived/implemented during the lecture or previous exercise - so feel free to borrow them from there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 2])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([-2,-1,0,1,2])\n",
    "np.maximum([0],a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    elementwise relu activation function\n",
    "    :param x: input to function [shape: arbitrary]\n",
    "    :return : relu(x) [shape: same as x]\n",
    "    \"\"\"\n",
    "    ### DONE #########################\n",
    "    return np.maximum([0],x)\n",
    "    ##################################\n",
    "\n",
    "\n",
    "def d_relu(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    elementwise gradient of relu activation function\n",
    "    :param x: input to function [shape: arbitrary]\n",
    "    :return : d relu(x) / dx [shape: same as x]\n",
    "    \"\"\"\n",
    "    ### DONE #########################\n",
    "    x[x<=0] = 0\n",
    "    x[x>0] = 1\n",
    "    return x\n",
    "    ##################################\n",
    "\n",
    "\n",
    "def sigmoid(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    elementwise sigmoid activation function\n",
    "    :param x: input to function [shape: arbitrary]\n",
    "    :return : d sigmoid(x) /dx [shape: same as x]\n",
    "    \"\"\"\n",
    "    ### DONE #########################\n",
    "    return (1/(1+np.exp(-x)))\n",
    "    ##################################\n",
    "\n",
    "\n",
    "def d_sigmoid(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    elementwise sigmoid activation function\n",
    "    :param x: input to function [shape: arbitrary]\n",
    "    :return : sigmoid(x) [shape: same as x]\n",
    "    \"\"\"\n",
    "    # --- Is this correct?\n",
    "    # --- Yes, seems so (https://towardsdatascience.com/derivative-of-the-sigmoid-function-536880cf918e)\n",
    "    ### DONE #########################\n",
    "    return sigmoid(x) * (1-sigmoid(x))\n",
    "    ##################################\n",
    "\n",
    "\n",
    "def binary_cross_entropy(predictions: np.ndarray, labels: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    binary cross entropy loss (negative bernoulli ll)\n",
    "    :param predictions: predictions by model (shape [N])\n",
    "    :param labels: class labels corresponding to train samples, (shape: [N])\n",
    "    :return binary cross entropy\n",
    "    \"\"\"\n",
    "    ### DONE #########################\n",
    "    N = labels.shape[0]\n",
    "    loss = np.zeros((N,1))\n",
    "    loss[labels == 1] = - np.log(predictions[labels == 1])\n",
    "    loss[labels == 0] = - np.log(1-predictions[labels == 0])\n",
    "    \n",
    "    # optional: divide by number of predictions/ labels\n",
    "    return 1/N * np.sum(loss)\n",
    "    ##################################\n",
    "\n",
    "\n",
    "def d_binary_cross_entropy(predictions: np.ndarray, labels: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    gradient of the binary cross entropy loss\n",
    "    :param predictions: predictions by model (shape [N])\n",
    "    :param labels: class labels corresponding to train samples, (shape [N])\n",
    "    :return gradient of binary cross entropy, w.r.t. the predictions (shape [N])\n",
    "    \"\"\"\n",
    "    ### DONE #########################\n",
    "    N = predictions.shape[0]\n",
    "    gradient = np.zeros(N)\n",
    "    gradient = (labels / predictions - ((1-labels) / (1-predictions)))\n",
    "    \n",
    "    # optional: divide by number of predictions/ labels\n",
    "    return -1/N * gradient\n",
    "    ##################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Setup & Intialization\n",
    "\n",
    "Next we are going to set up the Neural Network. We will represent it as a list of weight matrices and a list of bias vectors. Each list has one entry for each layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(neurons_per_hidden_layer: List[int], input_dim: int, output_dim: int, seed: int = 0) \\\n",
    "        -> Tuple[List[np.ndarray], List[np.ndarray]]:\n",
    "    \"\"\"\n",
    "    :param neurons_per_hidden_layer: list of numbers, indicating the number of neurons of each hidden layer\n",
    "    :param input_dim: input dimension of the network\n",
    "    :param output_dim: output dimension of the network\n",
    "    :param seed: seed for random number generator\n",
    "    :return list of weights and biases as specified by dimensions and hidden layer specification\n",
    "    \"\"\"\n",
    "    # seed random number generator\n",
    "    rng = np.random.RandomState(seed)\n",
    "    scale_factor = 1.0\n",
    "    prev_n = input_dim\n",
    "    weights = []\n",
    "    biases = []\n",
    "\n",
    "    # hidden layers\n",
    "    for n in neurons_per_hidden_layer:\n",
    "        # initialize weights with gaussian noise\n",
    "        weights.append(scale_factor * rng.normal(size=[prev_n, n]))\n",
    "        # initialize bias with zeros\n",
    "        biases.append(np.zeros([1, n]))\n",
    "        prev_n = n\n",
    "\n",
    "    # output layer\n",
    "    weights.append(scale_factor * rng.normal(size=[prev_n, output_dim]))\n",
    "    biases.append(np.zeros([1, output_dim]))\n",
    "\n",
    "    return weights, biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE** As NNs are non-convex, initialization plays a very important role in NN training and there is a lot of work into how to initialize them properly - this here is not a very good initialization, but sufficient for our small example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2) Forward Pass (3 p.)\n",
    "\n",
    "Next step is the forward pass, i.e., propagate a batch of samples through the network to get the final prediciton.\n",
    "But that's not all - to compute the gradietns later we also need to store all necessary quantities, here those are:\n",
    "- The input to every layer (here called h's)\n",
    "- The \"pre-activation\" of every layer, i.e., the qantity that is fed into the non-linearity (here called z's)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(x: np.ndarray, weights: List[np.ndarray], biases: List[np.ndarray])\\\n",
    "        -> Tuple[np.ndarray, List[np.ndarray], List[np.ndarray]]:\n",
    "    \"\"\"\n",
    "    propagate input through network\n",
    "    :param x: input: (shape, [N x input_dim])\n",
    "    :param weights: weight parameters of the layers\n",
    "    :param biases: bias parameters of the layers\n",
    "    :return: - Predictions of the network (shape, [N x out_put_dim])\n",
    "             - hs: output of each layer (input + all hidden layers) (length: len(weights))\n",
    "             - zs: preactivation of each layer (all hidden layers + output) (length: len(weights))\n",
    "    \"\"\"\n",
    "\n",
    "    hs = []  # list to store all inputs\n",
    "    zs = []  # list to store all pre-activations\n",
    "    \n",
    "    # input to first hidden layer is just the input to the network \n",
    "    h = x\n",
    "    hs.append(h)\n",
    "  \n",
    "    ### DONE #########################\n",
    "    # pass \"h\" to all hidden layers\n",
    "    # record all inputs and pre-activations in the lists  \n",
    "    for layer, w in enumerate(weights):\n",
    "        b = biases[layer]\n",
    "        \n",
    "        # weight shape: [2, 64], [64,64], [64, 1]\n",
    "        # data shape:   [100,2], [64,64], [64,64]\n",
    "        # bias shape:   [1, 64], [1, 64], [1, 1]\n",
    "        print(\"Layer\", layer, \"Weights\", w.shape, \"Data\", h.shape)\n",
    "        s = np.sum(w.T @ h.T,axis=0)\n",
    "        z = (s + b.T).T\n",
    "        print(s.shape, z.shape, b.shape)\n",
    "        zs.append(z)\n",
    "\n",
    "        h = relu(z)\n",
    "        print(\"H:\", h.shape)\n",
    "        hs.append(h)        \n",
    "        \n",
    "    ##################################\n",
    "    # has to have same shape as labels, i.e. [N,1]\n",
    "    y = sigmoid(z)   # z denotes the pre-activation of the output layer here. Feel free to rename it\n",
    "\n",
    "    return y, hs[:-1], zs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3) Backward Pass (4 p.)\n",
    "\n",
    "For training by gradient descent we need - well - gradients. Those are computed using backpropagation during the so called \"backward pass\". We will use the chain rule to propagate the gradient back through the network and at every layer, compute the gradients for the weights and biases at that layer. The initial gradient is given by the gradient of the loss function w.r.t. the network output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass(loss_grad: np.ndarray, \n",
    "                  hs: List[np.ndarray], zs: List[np.ndarray], \n",
    "                  weights: List[np.ndarray], biases: List[np.ndarray]) -> \\\n",
    "    Tuple[List[np.ndarray], List[np.ndarray]]:\n",
    "    \"\"\"\n",
    "    propagate gradient backwards through network\n",
    "    :param loss_grad: gradient of the loss function w.r.t. the network output (shape: [N, 1])\n",
    "    :param hs: values of all hidden layers during forward pass\n",
    "    :param zs: values of all preactivations during forward pass\n",
    "    :param weights: weight paramameters of the layers\n",
    "    :param biases: bias parameters of the layers\n",
    "    :return: d_weights: List of weight gradients - one entry with same shape for each entry of \"weights\"\n",
    "             d_biases: List of bias gradients - one entry with same shape for each entry of \"biases\"\n",
    "    \"\"\"\n",
    "\n",
    "    # return gradients as lists - we pre-initialize the lists as we iterate backwards\n",
    "    d_weights = [None] * len(weights)\n",
    "    d_biases  = [None] * len(biases)\n",
    "\n",
    "    ### TODO #########################\n",
    "    depth = len(weights)\n",
    "    hs_grad = [None] * (depth+1)\n",
    "    zs_grad = [None] * depth\n",
    "    \n",
    "    print(f'Weights: {depth} with shapes: {[w.shape for w in weights]}')\n",
    "    print(f'Biases: {depth} with shapes: {[b.shape for b in biases ]}')\n",
    "    \n",
    "    hs_grad[depth] = loss_grad # np.sum(loss_grad).reshape(-1,1)\n",
    "    \n",
    "    # backwards trough network -- [2, 1, 0] for 3 layers\n",
    "    for layer in range(depth-1, -1, -1):\n",
    "        \n",
    "        print(\"values:\", hs_grad[layer+1].shape, \"grad preact:\", d_sigmoid(zs[layer]).shape)\n",
    "        zs_grad[layer] = hs_grad[layer+1] * d_relu(zs[layer])\n",
    "        print(\"preactivation grad:\", zs_grad[layer].shape)\n",
    "        d_weights[layer] = np.outer(zs_grad[layer],hs[layer])\n",
    "        d_biases[layer] = zs_grad[layer]\n",
    "\n",
    "        # update hs_grad for next round -- like giving the loss to the subnetwork\n",
    "        print(\"Weights:\", weights[layer].shape, \"Preact Grad:\", zs_grad[layer].shape, \"\\n\")\n",
    "        hs_grad[layer] = (weights[layer] @ zs_grad[layer].T).T\n",
    "        \n",
    "    print(f'Weight Grad: {depth} with shapes: {[w.shape for w in d_weights]}')\n",
    "    print(f'Bias Grad: {depth} with shapes: {[b.shape for b in d_biases]}')\n",
    "        \n",
    "    ##################################\n",
    "\n",
    "    return d_weights, d_biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tying Everything Together \n",
    "\n",
    "Finally we can tie everything together and train our network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 Weights (2, 64) Data (100, 2)\n",
      "(100,) (100, 64) (1, 64)\n",
      "H: (100, 64)\n",
      "Layer 1 Weights (64, 64) Data (100, 64)\n",
      "(100,) (100, 64) (1, 64)\n",
      "H: (100, 64)\n",
      "Layer 2 Weights (64, 1) Data (100, 64)\n",
      "(100,) (100, 1) (1, 1)\n",
      "H: (100, 1)\n",
      "Weights: 3 with shapes: [(2, 64), (64, 64), (64, 1)]\n",
      "Biases: 3 with shapes: [(1, 64), (1, 64), (1, 1)]\n",
      "values: (100, 1) grad preact: (100, 1)\n",
      "preactivation grad: (100, 1)\n",
      "Weights: (64, 1) Preact Grad: (100, 1) \n",
      "\n",
      "values: (100, 64) grad preact: (100, 64)\n",
      "preactivation grad: (100, 64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vincent/.local/lib/python3.6/site-packages/ipykernel_launcher.py:32: RuntimeWarning: overflow encountered in exp\n",
      "/home/vincent/.local/lib/python3.6/site-packages/ipykernel_launcher.py:77: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: (64, 64) Preact Grad: (100, 64) \n",
      "\n",
      "values: (100, 64) grad preact: (100, 64)\n",
      "preactivation grad: (100, 64)\n",
      "Weights: (2, 64) Preact Grad: (100, 64) \n",
      "\n",
      "Weight Grad: 3 with shapes: [(6400, 200), (6400, 6400), (100, 6400)]\n",
      "Bias Grad: 3 with shapes: [(100, 64), (100, 64), (100, 1)]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (2,64) (6400,200) (2,64) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-84f03f73ba91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# apply gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mw_grads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mbiases\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mb_grads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (2,64) (6400,200) (2,64) "
     ]
    }
   ],
   "source": [
    "N = train_samples.shape[0]\n",
    "\n",
    "# hyper parameters \n",
    "layers = [64, 64]\n",
    "learning_rate = 1e-2\n",
    "\n",
    "# init model\n",
    "weights, biases = init_weights(layers, input_dim=2, output_dim=1, seed=42)\n",
    "\n",
    "\n",
    "#book keeping\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "# Here we work with a simple gradient descent implementation, using the whole dataset at each iteration,\n",
    "# You can modify it to stochastic gradient descent or a batch gradient descent procedure as an exercise\n",
    "for i in range(1000):\n",
    "    \n",
    "    # predict network outputs and record intermediate quantities using the forward pass\n",
    "    prediction, hs, zs = forward_pass(train_samples, weights, biases)\n",
    "    # print(\"Labels:\", train_labels.shape, \"vs. predictions\", prediction.shape)\n",
    "    train_losses.append(binary_cross_entropy(prediction, train_labels))\n",
    "\n",
    "    # compute gradients\n",
    "    loss_grad = d_binary_cross_entropy(prediction, train_labels)\n",
    "    w_grads, b_grads = backward_pass(loss_grad, hs, zs, weights, biases)\n",
    "\n",
    "    # apply gradients\n",
    "    for i in range(len(w_grads)):\n",
    "        weights[i] -= learning_rate * w_grads[i]\n",
    "        biases[i] -= learning_rate * b_grads[i]\n",
    "\n",
    "    test_losses.append(binary_cross_entropy(forward_pass(test_samples, weights, biases)[0], test_labels))\n",
    "\n",
    "# plotting\n",
    "plt.title(\"Loss\")\n",
    "plt.semilogy(train_losses)\n",
    "plt.semilogy(test_losses)\n",
    "plt.legend([\"Train Loss\", \"Test Loss\"])\n",
    "\n",
    "def plt_solution(samples, labels):\n",
    "    plt_range = np.arange(-1.5, 2.5, 0.01)\n",
    "    plt_grid = np.stack(np.meshgrid(plt_range, plt_range), axis=-1)\n",
    "    plt_grid_shape = plt_grid.shape[:2]\n",
    "    pred_grid = np.reshape(forward_pass(plt_grid, weights, biases)[0], plt_grid_shape)\n",
    "    plt.contour(plt_grid[..., 0], plt_grid[..., 1], pred_grid, levels=[0.5], colors=[\"black\"])\n",
    "    plt.contourf(plt_grid[..., 0], plt_grid[..., 1], pred_grid, levels=10)\n",
    "    plt.colorbar()\n",
    "    s0 = plt.scatter(x=samples[labels[:, 0] == 0, 0], y=samples[labels[:, 0] == 0, 1],\n",
    "                     label=\"c=0\", c=\"blue\")\n",
    "    s1 = plt.scatter(x=samples[labels[:, 0] == 1, 0], y=samples[labels[:, 0] == 1, 1],\n",
    "                     label=\"c=1\", c=\"orange\")\n",
    "    plt.legend([s0, s1], [\"c0\", \"c1\"])\n",
    "    plt.xlim(-1.5, 2.5)\n",
    "    plt.ylim(-1.5, 1.5)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Trained Network - with train samples\")\n",
    "plt_solution(train_samples, train_labels)\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Trained Network - with test samples\")\n",
    "plt_solution(test_samples, test_labels)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.ones((100,64))\n",
    "np.sum(a,axis=0).reshape(.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.) MNIST Classifier with PyTorch (10 p.)\n",
    "\n",
    "Modern deep learning approaches are mostly implemented using special libraries, providing functionality such as automatic differentiation, common SGD Optimiziers, easy usage of GPUs and so on. We will use PyTorch, at the moment the, arguably, most common framework (for research).\n",
    "\n",
    "## Getting Started\n",
    "You can find a documentation of the PyTorch API here https://pytorch.org/docs/stable/torch.html# . Don't worry if it seems a lot, we will point out the relevant bits during the exercise as we go along \n",
    "\n",
    "**Installation** \n",
    "You can find installation instructions here https://pytorch.org/ . Take the most recent stable version (1.7.X). We won't use GPUs here so you can take the cuda-free installation. We also don't need torchvision nor torchaudio so those don't need to be installed.\n",
    "\n",
    "**Data**\n",
    "We finally use a new dataset. The classical MNIST Handwritten Digit Classification set. It consists of grayscale images of size 28x28 of handwritten digits. Let's load it and visualize some of the images. We also do some preprocessing. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "data_dict = dict(np.load(\"mnist.npz\"))\n",
    "\n",
    "# prepare data:\n",
    "# - images are casted to float 32 (from uint8) mapped in interval (0,1) and a \"fake\" color channel is added.\n",
    "#   torch uses \"NCHW\"-layout for 2d convolutions. (i.e., a batch of images is represented as a 4 d tensor \n",
    "#   where the first axis (N) is the batch dimension, the second the (color) **C**hannels, followed by a **H**eight\n",
    "#   and a **W**idth axis). As we have grayscale images there is only 1 color channel.  \n",
    "# - targets are mapped to one hot encoding - torch does that for us \n",
    "\n",
    "\n",
    "with torch.no_grad(): \n",
    "    # YTA: We don't need torch to calculate gradients here since we only want to transform data and not compute anything\n",
    "    # YTA: reshape() because of: https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html\n",
    "    train_samples = torch.from_numpy(data_dict[\"train_samples\"].astype(np.float32) / 255.0).reshape(-1, 1, 28, 28)\n",
    "    train_labels = torch.nn.functional.one_hot(torch.from_numpy(data_dict[\"train_labels\"]))\n",
    "    test_samples = torch.from_numpy(data_dict[\"test_samples\"].astype(np.float32) / 255.0).reshape(-1, 1, 28, 28)\n",
    "    test_labels = torch.nn.functional.one_hot(torch.from_numpy(data_dict[\"test_labels\"]))\n",
    "\n",
    "\n",
    "# plot first 25 images in train setp\n",
    "plt.figure(figsize=(25, 1))\n",
    "for i in range(25):\n",
    "    plt.subplot(1, 25, i + 1)\n",
    "    # drop channel axis for plotting\n",
    "    plt.imshow(train_samples[i, 0], cmap=\"gray\", interpolation=\"none\")\n",
    "    plt.gca().axis(\"off\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1) Specifiying Networks (4 p.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in training a neural network is specifying its architecture. Here we will actually build two networks\n",
    "- classifier_fc: A classifier consisting only of fully connected layers\n",
    "- classifier_conv: A classifier combining, convolutional layers, pooling and fully connected layers\n",
    "\n",
    "In the torch API under torch.nn you can find everything you need. Take a look at the classes \"Linear\", \"ReLU\", \"Softmax\", \"Conv2d\", \"MaxPool2d\" and \"Sequential\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_fc = [\n",
    "    torch.nn.Flatten(), # Flatten image into vector\n",
    "    ## TODO ##\n",
    "    # Hidden Layer 1: 256 neurons, Relu activation\n",
    "    nn.Linear(28**2,256),\n",
    "    nn.ReLU(),\n",
    "    \n",
    "    # Hidden Layer 2: 128 neurons, Relu activation\n",
    "    nn.Linear(256,128),\n",
    "    nn.ReLU(),\n",
    "    \n",
    "    # Outputlayer: 10 neurons (one for each class), softmax activation\n",
    "    nn.Linear(128,10),\n",
    "    nn.Softmax()\n",
    "    ###########\n",
    "]\n",
    "classifier_fc = torch.nn.Sequential(\n",
    "    *layers_fc # unpack layers\n",
    ")\n",
    "\n",
    "# YTA: THIS IS NOT DONE\n",
    "layers_conv = [\n",
    "    # Conv Layer 1: 8 filters of 3x3 size, ReLU, Max Pool with size 2x2 and stride 2\n",
    "    nn.Conv2d(8, 33, 3, stride=2), # 1 auf 8 \n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(22,2),\n",
    "    \n",
    "    # Conv Layer 2: 16 filters of 3x3 size, ReLU, Max Pool with size 2x2 and stride 2\n",
    "    nn.Conv2d(16, 33, 3, stride=2), # 8 auf 16\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(22,2),\n",
    "    \n",
    "    # Flatten\n",
    "    nn.Flatten(),\n",
    "    \n",
    "    # Fully Connected Layer 1: 64 Neurons, ReLU\n",
    "    nn.Linear(400, 64),\n",
    "    nn.ReLU(),\n",
    "    \n",
    "    # Outputlayer: 10 neurons (one for each class), softmax activation\n",
    "    nn.Linear(64,10),\n",
    "    nn.Softmax()\n",
    "]\n",
    "classifier_conv = torch.nn.Sequential(\n",
    "    *layers_conv\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From now on we going to use both the classifiers interchangable, pick one here and the rest should work with both models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = classifier_fc \n",
    "#classifier = classifier_conv "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2) Optimizer and Loss (2 p.)\n",
    "\n",
    "Next we need to specify an optimizer and a loss function. For the optimizer we will use Adam (look at torch.optim) with default parameters and as a loss function we will use the cross-entropy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "optimizer:torch.optim.Adam = Adam(classifier.parameters(), lr=1e-4)\n",
    "\n",
    "def cross_entropy_loss(labels: torch.Tensor, predictions: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\" Cross entropy Loss:\n",
    "    :param labels: Ground truth class labels (shape; [N, num_classes])\n",
    "    :param predictions: predicted class labels (shape: [N, num_classes])\n",
    "    :return: cross entropy (scalar)\n",
    "    \"\"\"\n",
    "    # YTA: I am ~80% sure we should implement this ourselves but I am learning PyTorch with this\n",
    "    loss = CrossEntropyLoss()\n",
    "    print(predictions.shape, labels.shape)\n",
    "    return loss(predictions, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3) Data Loader (2 p.) \n",
    "\n",
    "For batch gradient descent we need to shuffle and batch the data, PyTorch provdies some functionality for that in form of the \"DataLoader\" (Look at torch.utils.data). In the simplest form used here, it simply shuffles and batches the data but you can also build more complex preprocessing pipelines. We also need a loader for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-9c783d532527>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#TODO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain_data\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtest_data\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *tensors)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not callable"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "#TODO \n",
    "train_data   = TensorDataset(train_samples, train_labels)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "test_data    = TensorDataset(test_samples, test_labels)\n",
    "test_loader  = DataLoader(test_data, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "#####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4) Training (2 p.)\n",
    "\n",
    "We now have all ingredients and can implement our train loop and a evaluation procedure. You should get a test set accuracy of > 0.95 with both architectures in 2 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001\n",
      "torch.Size([64, 10]) torch.Size([64, 10])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "1D target tensor expected, multi-target not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-7f29c3f5b91f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# YTA: note for the future: https://discuss.pytorch.org/t/runtimeerror-multi-target-not-supported-newbie/10216/2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# clear gradient buffers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-055bca1060a8>\u001b[0m in \u001b[0;36mcross_entropy_loss\u001b[0;34m(labels, predictions)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    960\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m--> 962\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2466\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2467\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2468\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2262\u001b[0m                          .format(input.size(0), target.size(0)))\n\u001b[1;32m   2263\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2264\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2265\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2266\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: 1D target tensor expected, multi-target not supported"
     ]
    }
   ],
   "source": [
    "epochs = 2  # small number of epochs should be sufficient to get descent performance\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "\n",
    "for i in range(epochs):\n",
    "    print(\"Epoch {:03d}\".format(i + 1))\n",
    "    for batch in train_loader:\n",
    "        #TODO################## \n",
    "        # forward pass        #\n",
    "        samples, labels = batch\n",
    "        predictions = classifier(samples)\n",
    "        \n",
    "        # backward pass       #\n",
    "        \n",
    "        # YTA: note for the future: https://discuss.pytorch.org/t/runtimeerror-multi-target-not-supported-newbie/10216/2\n",
    "        loss = cross_entropy_loss(labels, predictions)\n",
    "        optimizer.zero_grad() # clear gradient buffers\n",
    "        loss.backward()\n",
    "        \n",
    "        # update step\n",
    "        optimizer.step()\n",
    "        \n",
    "        #######################\n",
    "        train_losses.append(loss.detach().numpy())\n",
    "\n",
    "# Evaluate (we still need batching as evaluating all test points at once would probably melt your Memory)\n",
    "avg_loss = avg_acc = 0\n",
    "for batch in test_loader:\n",
    "    samples, labels = batch\n",
    "    predictions = classifier(samples)\n",
    "    loss = cross_entropy_loss(labels, predictions)\n",
    "    acc = torch.count_nonzero(predictions.argmax(dim=-1) == labels.argmax(dim=-1)) / samples.shape[0]\n",
    "\n",
    "    avg_acc += acc / len(test_loader)\n",
    "    avg_loss += loss / len(test_loader)\n",
    "\n",
    "\n",
    "print(\"Test Set Accuracy: {:.3f}, Test Loss {:.3f}\".format(avg_acc.detach().numpy(), avg_loss.detach().numpy()))\n",
    "\n",
    "plt.figure()\n",
    "plt.semilogy(train_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
