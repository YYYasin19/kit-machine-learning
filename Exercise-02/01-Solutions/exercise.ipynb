{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXERCISE 2  - ML - Grundverfahren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.) Multiclass Classification\n",
    "\n",
    "The Iris Dataset is a very classical machine learning and statistics benchmark for classification, developed in the 1930's. The goal is to classify 3 types of flowers (more specifically, 3 types of flowers form the Iris species) based on 4 features: petal length, petal width, sepal length and sepal width.\n",
    "\n",
    "As we have $K=3$ different types of flowers we are dealing with a multi-class classification problem and need to extend our sigmoid-based classifier from before the previous exercise / lecture notebook. \n",
    "\n",
    "We will reuse our \"minimize\" and \"affine feature\" functions. Those are exactly as before. The affine feautres are sufficient here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/ytatar/Yasin/01-Projects/20-11-06-KIT-WS2020-ML-Grundverfahren/kit-machine-learning/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in /Users/ytatar/Yasin/01-Projects/20-11-06-KIT-WS2020-ML-Grundverfahren/kit-machine-learning/venv/lib/python3.7/site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /Users/ytatar/Yasin/01-Projects/20-11-06-KIT-WS2020-ML-Grundverfahren/kit-machine-learning/venv/lib/python3.7/site-packages (from sklearn) (0.23.2)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /Users/ytatar/Yasin/01-Projects/20-11-06-KIT-WS2020-ML-Grundverfahren/kit-machine-learning/venv/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.19.4)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /Users/ytatar/Yasin/01-Projects/20-11-06-KIT-WS2020-ML-Grundverfahren/kit-machine-learning/venv/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.5.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/ytatar/Yasin/01-Projects/20-11-06-KIT-WS2020-ML-Grundverfahren/kit-machine-learning/venv/lib/python3.7/site-packages (from scikit-learn->sklearn) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/ytatar/Yasin/01-Projects/20-11-06-KIT-WS2020-ML-Grundverfahren/kit-machine-learning/venv/lib/python3.7/site-packages (from scikit-learn->sklearn) (0.17.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /Users/ytatar/Yasin/01-Projects/20-11-06-KIT-WS2020-ML-Grundverfahren/kit-machine-learning/venv/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.19.4)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "!pip install sklearn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "from typing import Callable, Tuple\n",
    "from rich.traceback import install\n",
    "install()\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def minimize(f: Callable , df: Callable, x0: np.ndarray, lr: float, num_iters: int) -> \\\n",
    "        Tuple[np.ndarray, float, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    :param f: objective function\n",
    "    :param df: gradient of objective function\n",
    "    :param x0: start point, shape [dimension]\n",
    "    :param lr: learning rate\n",
    "    :param num_iters: maximum number of iterations\n",
    "    :return argmin, min, values of x for all interations, value of f(x) for all iterations\n",
    "    \"\"\"\n",
    "    # initialize\n",
    "    x = np.zeros([num_iters + 1] + list(x0.shape))\n",
    "    f_x = np.zeros(num_iters + 1)\n",
    "    x[0] = x0\n",
    "    f_x[0] = f(x0)\n",
    "    for i in range(num_iters):\n",
    "        # update using gradient descent rule\n",
    "        grad = df(x[i])\n",
    "        x[i + 1] = x[i] - lr * grad\n",
    "        f_x[i + 1] = f(x[i + 1])\n",
    "    return x[i+1], f_x[i+1], x[:i+1], f_x[:i+1] # logging info for visualization\n",
    "\n",
    "# 1 + t * x_1\n",
    "def affine_features(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    implements affine feature function\n",
    "    :param x: inputs\n",
    "    :return inputs with additional bias dimension\n",
    "    \"\"\"\n",
    "    return np.concatenate([x, np.ones((x.shape[0], 1))], axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Prepare Data\n",
    "In the original dataset the different types of flowers are labeled with $0, 1$ and $2$. The output of our classifier will be a vector with $K=3$ entries, $\\begin{pmatrix}p(c=0) & p(c=1) & p(c=2) \\end{pmatrix}$, i.e. the probability for each class that a given sample is an instance of that class. Thus we need to represent the labels in a different form, a so called one-hot encoding. This is  vector of the length of number of classes, in this case 3, with zeros everywhere except for the entry corresponding to the class number, which is one. For the train and test data we know to which class it belongs, so the probability for that class is one and the probability for all other classes zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = np.load(\"iris_data.npz\")\n",
    "train_samples = data[\"train_features\"]\n",
    "train_labels = data[\"train_labels\"]\n",
    "test_samples = data[\"test_features\"]\n",
    "test_labels = data[\"test_labels\"]\n",
    "\n",
    "train_features = affine_features(train_samples)\n",
    "test_features = affine_features(test_samples)\n",
    "\n",
    "def generate_one_hot_encoding(y: np.ndarray, num_classes: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    :param y: vector containing classes as numbers, shape: [N]\n",
    "    :param num_classes: number of classes\n",
    "    :return a matrix containing the labels in an one-hot encoding, shape: [N x K]\n",
    "    \"\"\"\n",
    "    y_oh = np.zeros([y.shape[0], num_classes])\n",
    "\n",
    "    # can be done more efficiently using numpy with\n",
    "    # y_oh[np.arange(y.size), y] = 1.0\n",
    "    # but I decided to used the for loop for clarity\n",
    "\n",
    "    for i in range(y.shape[0]):\n",
    "        y_oh[i, y[i]] = 1.0\n",
    "\n",
    "    return y_oh\n",
    "\n",
    "\n",
    "oh_train_labels = generate_one_hot_encoding(train_labels, 3)\n",
    "oh_test_labels = generate_one_hot_encoding(test_labels, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization using Gradient Descent\n",
    "\n",
    "The multi-class generalization of the sigmoid is the softmax function. It takes an vector of length $K$ and outputs another vector of length $K$ where the $k-th$ entry is given by\n",
    "$$ \\textrm{softmax}(\\boldsymbol{x})_k = \\dfrac{\\exp(x_k)}{\\sum_{j=1}^K \\exp(x_j)}.$$\n",
    "The output vector always sumes to $1$, thus can be interpreted as parameters of a categorical distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def softmax(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"softmax function\n",
    "    :param x: inputs, shape: [N x K]\n",
    "    :return softmax(x), shape [N x K]\n",
    "    \"\"\"\n",
    "    a = np.max(x, axis=-1, keepdims=True)\n",
    "    log_normalizer = a + np.log(np.sum(np.exp(x - a), axis=-1, keepdims=True))\n",
    "    return np.exp(x - log_normalizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Practical Aspect:** In the above implementation of the softmax we stayed in the log-domain until the very last command.\n",
    "We also used the log-sum-exp-trick (https://en.wikipedia.org/wiki/LogSumExp#log-sum-exp_trick_for_log-domain_calculations).\n",
    "Staying in the log domain and applying the log-sum-exp-trick whenever possible is a simple way to make the implementation\n",
    "more numerically robust. It does not change anything with regards to the underlying theory.\n",
    "\n",
    "We also need to extend our loss function. Instead of the likelihood of a bernoulli, we now maximize the likelihood of a categorical distribution which, for a single sample $\\boldsymbol{x}_i$ is given by\n",
    "$$  \\sum_k^K h_{i, k} \\log(p_{i,k}) $$\n",
    "where $h_i$ denotes the true label and $p_i$ the one predicted by the classifier (both in one-hot representation).\n",
    "As we are again minimizing we instead implement the negated likelihood of a categorical distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def categorical_nll(predictions: np.ndarray, labels: np.ndarray, epsilon: float = 1e-12) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    cross entropy loss function\n",
    "    :param predictions: class labels predicted by the classifier, shape: [N x K]\n",
    "    :param labels: true class labels, shape: [N x K]\n",
    "    :param epsilon: small offset to avoid numerical instabilities (i.e log(0))\n",
    "    :return negative log-likelihood of the labels given the predictions, shape: [N]\n",
    "    \"\"\"\n",
    "    return - np.sum(labels * np.log(predictions + epsilon), -1) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we work with the mean over all samples instead of the sum thus, the full loss if given by.\n",
    "\n",
    "\\begin{align} Loss_{cat\\_nll} &= - \\sum_k^K h_{i, k} \\log(p_{i,k}) = - \\dfrac{1}{N} \\sum_i^N  \\sum_k^K h_{i, k} \\log(\\textrm{softmax}(\\boldsymbol{w}^T \\phi(\\boldsymbol{x}_i))_k) \\\\ &\n",
    "= - \\dfrac{1}{N} \\sum_i^N \\left(\\sum_k^K h_{i,k}\\left(\\boldsymbol{w}^T_k\\phi(\\boldsymbol{x}_i) - \\log \\sum_j^K \\exp(\\boldsymbol{w}_j^T \\phi(\\boldsymbol{x}_i))\\right)\\right)\n",
    "\\end{align}\n",
    "\n",
    "### 1.1) Derivation (4 Points)\n",
    "Derive the gradient of the loss function w.r.t. $w$, i.e., $\\dfrac{\\partial Loss_{cat\\_nll}}{\\partial \\boldsymbol{w}}$\n",
    "\n",
    "**Hint 1:** Follow the steps in the derivation of the gradient of the loss for the binary classification in the lecture\n",
    "\n",
    "**Hint 2:** Derive the gradient not for the whole matrix $w$ but only for the vector $w_k$, i.e., the weights\n",
    "corresponding to class $k$, i.e., $\\dfrac{\\partial Loss_{cat\\_nll}}{\\partial \\boldsymbol{w}_k}$. The gradients for the individual\n",
    "$w_k$ can be \"stacked\" to obtain the full gradient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.codecogs.com/eqnedit.php?latex=\\dpi{100}&space;\\begin{align*}&space;\\frac{\\partial&space;loss}{\\partial&space;w_k}&space;&=&space;-\\frac{1}{N}\\sum_i^N&space;\\left(\\sum_k^K&space;h_{i,k}&space;\\left(\\frac{\\partial}{\\partial&space;w_k}&space;\\left(w_k^T\\phi(x_i)-\\log\\sum_j^K&space;\\exp(w_j^T&space;\\phi(x_i))&space;\\right&space;)&space;\\right&space;)&space;\\right)&space;&&&space;\\text{Derivative&space;can&space;be&space;dragged&space;into&space;the&space;inside}&space;\\\\&space;&=&space;-\\frac{1}{N}\\sum_i^N&space;\\left(\\sum_k^K&space;h_{i,k}&space;\\left(\\frac{\\partial}{\\partial&space;w_k}&space;(w_k^T\\phi(x_i))&space;-&space;\\frac{\\partial}{\\partial&space;w_k}&space;\\left(\\log\\sum_j^K&space;\\exp(w_j^T&space;\\phi(x_i))&space;\\right&space;)&space;\\right&space;)&space;\\right)&space;\\\\&space;&=&space;-\\frac{1}{N}\\sum_i^N&space;\\left(\\sum_k^K&space;h_{i,k}&space;\\left(\\phi(x_i)^T&space;-&space;\\frac{1}{\\sum_j^K&space;\\exp(w_j^T&space;\\phi(x_i))}&space;*&space;\\exp(w_j^T&space;\\phi(x_i))&space;*&space;\\phi(x_i)&space;\\right&space;)&space;\\right)&space;&&&space;\\text{The&space;right&space;part&space;looks&space;like&space;the&space;softmax&space;function&space;for&space;the&space;i-th&space;sample}&space;\\\\&space;&=&space;-\\frac{1}{N}\\sum_i^N&space;\\left(\\sum_k^K&space;h_{i,k}&space;\\left(\\phi(x_i)^T&space;-&space;\\text{softmax}(c=k|x_i)&space;*&space;\\phi(x_i)&space;\\right&space;)&space;\\right)&space;\\end{align*}\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\dpi{100}&space;\\begin{align*}&space;\\frac{\\partial&space;loss}{\\partial&space;w_k}&space;&=&space;-\\frac{1}{N}\\sum_i^N&space;\\left(\\sum_k^K&space;h_{i,k}&space;\\left(\\frac{\\partial}{\\partial&space;w_k}&space;\\left(w_k^T\\phi(x_i)-\\log\\sum_j^K&space;\\exp(w_j^T&space;\\phi(x_i))&space;\\right&space;)&space;\\right&space;)&space;\\right)&space;&&&space;\\text{Derivative&space;can&space;be&space;dragged&space;into&space;the&space;inside}&space;\\\\&space;&=&space;-\\frac{1}{N}\\sum_i^N&space;\\left(\\sum_k^K&space;h_{i,k}&space;\\left(\\frac{\\partial}{\\partial&space;w_k}&space;(w_k^T\\phi(x_i))&space;-&space;\\frac{\\partial}{\\partial&space;w_k}&space;\\left(\\log\\sum_j^K&space;\\exp(w_j^T&space;\\phi(x_i))&space;\\right&space;)&space;\\right&space;)&space;\\right)&space;\\\\&space;&=&space;-\\frac{1}{N}\\sum_i^N&space;\\left(\\sum_k^K&space;h_{i,k}&space;\\left(\\phi(x_i)^T&space;-&space;\\frac{1}{\\sum_j^K&space;\\exp(w_j^T&space;\\phi(x_i))}&space;*&space;\\exp(w_j^T&space;\\phi(x_i))&space;*&space;\\phi(x_i)&space;\\right&space;)&space;\\right)&space;&&&space;\\text{The&space;right&space;part&space;looks&space;like&space;the&space;softmax&space;function&space;for&space;the&space;i-th&space;sample}&space;\\\\&space;&=&space;-\\frac{1}{N}\\sum_i^N&space;\\left(\\sum_k^K&space;h_{i,k}&space;\\left(\\phi(x_i)^T&space;-&space;\\text{softmax}(c=k|x_i)&space;*&space;\\phi(x_i)&space;\\right&space;)&space;\\right)&space;\\end{align*}\" title=\"\\begin{align*} \\frac{\\partial loss}{\\partial w_k} &= -\\frac{1}{N}\\sum_i^N \\left(\\sum_k^K h_{i,k} \\left(\\frac{\\partial}{\\partial w_k} \\left(w_k^T\\phi(x_i)-\\log\\sum_j^K \\exp(w_j^T \\phi(x_i)) \\right ) \\right ) \\right) && \\text{Derivative can be dragged into the inside} \\\\ &= -\\frac{1}{N}\\sum_i^N \\left(\\sum_k^K h_{i,k} \\left(\\frac{\\partial}{\\partial w_k} (w_k^T\\phi(x_i)) - \\frac{\\partial}{\\partial w_k} \\left(\\log\\sum_j^K \\exp(w_j^T \\phi(x_i)) \\right ) \\right ) \\right) \\\\ &= -\\frac{1}{N}\\sum_i^N \\left(\\sum_k^K h_{i,k} \\left(\\phi(x_i)^T - \\frac{1}{\\sum_j^K \\exp(w_j^T \\phi(x_i))} * \\exp(w_j^T \\phi(x_i)) * \\phi(x_i) \\right ) \\right) && \\text{The right part looks like the softmax function for the i-th sample} \\\\ &= -\\frac{1}{N}\\sum_i^N \\left(\\sum_k^K h_{i,k} \\left(\\phi(x_i)^T - \\text{softmax}(c=k|x_i) * \\phi(x_i) \\right ) \\right) \\end{align*}\" /></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*} \\frac{\\partial loss}{\\partial w_k} \n",
    "&= -\\frac{1}{N}\\sum_i^N \\left(\\sum_k^K h_{i,k} \\left(\\frac{\\partial}{\\partial w_k} \\left(w_k^T\\phi(x_i)-\\log\\sum_j^K \\exp(w_j^T \\phi(x_i)) \\right ) \\right ) \\right) && \\text{Derivative can be dragged into the inside} \\\\ \n",
    "&= -\\frac{1}{N}\\sum_i^N \\left(\\sum_k^K h_{i,k} \\left(\\frac{\\partial}{\\partial w_k} (w_k^T\\phi(x_i)) - \\frac{\\partial}{\\partial w_k} \\left(\\log\\sum_j^K \\exp(w_j^T \\phi(x_i)) \\right ) \\right ) \\right) \\\\ \n",
    "&= -\\frac{1}{N}\\sum_i^N \\left(\\sum_k^K h_{i,k} \\left(\\phi(x_i)^T - \\frac{1}{\\sum_j^K \\exp(w_j^T \\phi(x_i))} * \\exp(w_k^T \\phi(x_i)) * \\phi(x_i) \\right ) \\right) && \\text{The right part looks like the softmax function for the i-th sample} \\\\ \n",
    "&= -\\frac{1}{N}\\sum_i^N \\left(\\sum_k^K h_{i,k} \\left(\\phi(x_i)^T - \\text{softmax}(c=k|w^T\\phi(x_i)) * \\phi(x_i) \\right ) \\right) \n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2) Implementation (3 Points)\n",
    "Fill in the function skeletons below so that they implement the loss and its gradient.\n",
    "\n",
    "**Hint:** The optimizer works with vectors only. So the function get the weights as vecotrs in the flat_weights parameter.\n",
    "Make sure you reshape them appropriately before using them for the computations. For the gradients make sure to return\n",
    "again a vector by flattening the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# objective\n",
    "def objective_cat(flat_weights: np.ndarray, features: np.ndarray, labels: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    :param flat_weights: weights of the classifier (as flattened vector), shape: [feature_dim * K]\n",
    "    :param features: samples to evaluate objective on, shape: [N x feature_dim]\n",
    "    :param labels: labels corresponding to samples, shape: [N]\n",
    "    :return cross entropy loss of the classifier given the samples \n",
    "    \"\"\"\n",
    "    num_features = features.shape[-1]\n",
    "    num_classes = labels.shape[-1]\n",
    "    weights = np.reshape(flat_weights, [num_features, num_classes])\n",
    "    #---------------------------------------------------------------\n",
    "    # TODO\n",
    "    return np.sum(categorical_nll(softmax(features @ weights), labels))\n",
    "    #---------------------------------------------------------------\n",
    "\n",
    "\n",
    "# derivative\n",
    "def d_objective_cat(flat_weights: np.ndarray, features: np.ndarray, labels: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    :param flat_weights: weights of the classifier (as flattened vector), shape: [feature_dim * K]\n",
    "    :param features: samples to evaluate objective on, shape: [N x feature_dim]\n",
    "    :param labels: labels corresponding to samples, shape: [N]\n",
    "    :return gradient of cross entropy loss of the classifier given the samples, shape: [feature_dim * K]\n",
    "    \"\"\"\n",
    "    feature_dim = features.shape[-1]\n",
    "    num_classes = labels.shape[-1]\n",
    "\n",
    "    weights = np.reshape(flat_weights, [feature_dim, num_classes])\n",
    "    #---------------------------------------------------------------\n",
    "    # TODO, do not forget to flatten the gradient before returning!\n",
    "    # YTA: because our w0_flat is all rows stacked side-by-side\n",
    "    num_samples = features.shape[0]\n",
    "    d_weights = np.zeros((num_classes,feature_dim))\n",
    "    for k in range(0,num_classes):\n",
    "        # get current labels and weights\n",
    "        clabels = labels[:,k]\n",
    "        predictions_k = features @ weights[:,k]\n",
    "        # d_weights[k] = - (1/num_samples) * (clabels.T @ features - (softmax(predictions_k).T @ features))\n",
    "        d_weights[k] = - (1/num_samples) * np.sum(features.T)\n",
    "        pass\n",
    "    \n",
    "    return d_weights.flatten()\n",
    "    #---------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can tie everything together again. Both train and test accuracy should be at least 0.9:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Loss: 1105.2635189849059\n",
      "Train Accuracy: 0.6666666666666666 Test Accuracy: 0.6666666666666666\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "w0_flat = np.zeros(5*3) # 4 features + bias, 3 classes\n",
    "w_opt_flat, loss_opt, x_history, f_x_history = \\\n",
    "   minimize(lambda w: objective_cat(w, train_features, oh_train_labels),\n",
    "            lambda w: d_objective_cat(w, train_features, oh_train_labels),\n",
    "            w0_flat, 1e-2, 2000)\n",
    "\n",
    "w_opt = np.reshape(w_opt_flat, [5, 3])\n",
    "\n",
    "# plotting and evaluation\n",
    "print(\"Final Loss:\", loss_opt)\n",
    "plt.figure()\n",
    "plt.plot(f_x_history)\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"negative categorical log-likelihood\")\n",
    "\n",
    "train_pred = softmax(train_features @ w_opt)\n",
    "train_acc = np.count_nonzero(np.argmax(train_pred, axis=-1) == np.argmax(oh_train_labels, axis=-1))\n",
    "train_acc /= train_labels.shape[0]\n",
    "test_pred = softmax(test_features @ w_opt)\n",
    "test_acc = np.count_nonzero(np.argmax(test_pred, axis=-1) == np.argmax(oh_test_labels, axis=-1))\n",
    "test_acc /= test_labels.shape[0]\n",
    "print(\"Train Accuracy:\", train_acc, \"Test Accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.) k-NN (3 Points) (DONE)\n",
    "\n",
    "Here we implement a simple k-NN appraoch. As we want to use it for classification now and later for regression we choose a modular appraoch and first implement a function that returns the $k$ nearest values and corresponding target to a given querry point. We than also implement a function doing a majority vote for classification, given the k nearest targets. Note that we use the \"real\" labels, not the one-hot encoding for the k-NN classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k_nearest(k: int, query_point: np.ndarray, x_data: np.ndarray, y_data: np.ndarray) \\\n",
    "    -> Tuple[np.ndarray, np.ndarray]:                                                                                        \n",
    "    \"\"\"\n",
    "    :param k: number of nearest neigbours to return \n",
    "    :param query_point: point to evaluate, shape [input_dimension]\n",
    "    :param x_data: x values of the data [N x input_dimension]\n",
    "    :param y_data: y values of the data [N x target_dimension]\n",
    "    :return k-nearest x values [k x input_dimension], k-nearest y values [k x target_dimension]\n",
    "    \"\"\"\n",
    "    #---------------------------------------------------------------\n",
    "    # Explanation: Calculates distances for every point\n",
    "    # Sort distances array and get indices with np.argsort()\n",
    "    # Access x_data and y_data with exactly these indices and pick the first k values\n",
    "    \n",
    "    distances = np.sum(np.abs(x_data-query_point)**2,axis=-1)**(1./2)\n",
    "    return x_data[np.argsort(distances)][:k], y_data[np.argsort(distances)][:k]\n",
    "    #---------------------------------------------------------------\n",
    "\n",
    "# y = [1, 0, 0, 1, 1] -> 1\n",
    "def majority_vote(y: np.ndarray) -> int:\n",
    "    \"\"\"\n",
    "    :param y: k nearest targets [K]\n",
    "    :return the number x which occours most often in y. \n",
    "    \"\"\"\n",
    "    #---------------------------------------------------------------\n",
    "    # TODO\n",
    "    return np.argmax(np.bincount(y))\n",
    "    #---------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the classifier and measure the accuracy. For $k=5$ it should be $1.0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "k = 5\n",
    "\n",
    "predictions = np.zeros(test_features.shape[0])\n",
    "for i in range(test_features.shape[0]):\n",
    "    _, nearest_y = get_k_nearest(k, test_features[i], train_features, train_labels)\n",
    "    predictions[i] = majority_vote(nearest_y)\n",
    "\n",
    "print(\"Accuracy: \", np.count_nonzero(predictions == test_labels) / test_labels.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.) Hold - out and Cross Validation\n",
    "In this part of the exercise we will have a closer look on Hould-out and Cross Validation for model selection. We will apply them on different regression algorithms. <br>\n",
    "Let's first have a look on the data. Please note that the data is given as a tensor: [20 x 50 x 1], corresponding to 20 different data sets with 50 data points in each. The data is 1 dimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(33)\n",
    "# We have 20 training sets with 50 samples in each set\n",
    "x_samples = np.load('x_samples.npy')    # shape: [20, 50, 1]\n",
    "y_samples = np.load('y_samples.npy')    # shape: [20, 50, 1]\n",
    "\n",
    "\n",
    "# we load our plot data (shape: [20, 1])\n",
    "x_plt = np.load('x_plt.npy')\n",
    "y_plt = np.load('y_plt.npy')\n",
    "\n",
    "# let's plot our data (for the training data we just use the first training set)\n",
    "plt.plot(x_plt, y_plt, c=\"blue\", label=\"Ground Truth Polynomial\")\n",
    "plt.scatter(x_samples[0, :, :], y_samples[0, :, :], c=\"orange\", label=\"Samples of first trianing set\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions for Plotting\n",
    "Before we start, we define some functions here which we will make use of. You do not need to implement anything in here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(mse_val: np.ndarray, mse_train:np.ndarray, x_axis, m_star_idx:int, x_plt:np.ndarray, y_plt: np.ndarray,\n",
    "        x_samples:np.ndarray, y_samples:np.ndarray, model_best, model_predict_func:callable):\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(121)\n",
    "    plot_error_curves(mse_val, mse_train, x_axis, m_star_idx)\n",
    "    plt.subplot(122)\n",
    "    plot_best_model(x_plt, y_plt, x_samples, y_samples, model_best, model_predict_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_error_curves(MSE_val: np.ndarray, MSE_train:np.ndarray, x_axis, m_star_idx: int):\n",
    "    plt.yscale('log')\n",
    "    plt.plot(x_axis, np.mean(MSE_val, axis=0), color='blue', alpha=1, label=\"mean MSE validation\")\n",
    "    plt.plot(x_axis, np.mean(MSE_train, axis=0), color='orange', alpha=1, label=\"mean MSE train\")\n",
    "    plt.plot(x_axis[m_star_idx], np.min(np.mean(MSE_val, axis=0)), \"x\", label='best model')\n",
    "    plt.xticks(x_axis)\n",
    "    plt.xlabel(\"model order\")\n",
    "    plt.ylabel(\"MSE\")\n",
    "    plt.legend()\n",
    "    \n",
    "def plot_best_model(x_plt: np.ndarray, y_plt: np.ndarray, x_samples: np.ndarray, y_samples:np.ndarray, \n",
    "                    model_best, model_predict_func: callable):\n",
    "    plt.plot(x_plt, y_plt, color='g', label=\"Ground truth\")\n",
    "    plt.scatter(x_samples, y_samples, label=\"Noisy data\", color=\"orange\")\n",
    "    f_hat = model_predict_func(model_best, x_plt)\n",
    "    plt.plot(x_plt, f_hat, label=\"Best model\")\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.legend()\n",
    "    \n",
    "def plot_bars(M, std_mse_val_ho, std_mse_val_cv):\n",
    "    models = np.arange(1, M+1)\n",
    "    fig = plt.figure(\"Comparison of the Standard Deviations of mse's\")\n",
    "    ax1 = fig.add_subplot(111)\n",
    "    ax1.bar(models, std_mse_val_ho, yerr=np.zeros(std_mse_val_ho.shape), align='center', alpha=0.5, ecolor='black', \n",
    "               color='red', capsize=None)\n",
    "    ax1.bar(models, std_mse_val_cv, yerr=np.zeros(std_mse_val_cv.shape), align='center', alpha=0.5, ecolor='black', \n",
    "               color='blue', capsize=None)\n",
    "    ax1.set_xticks(models)\n",
    "    ax1.set_xlabel('Model')\n",
    "    ax1.set_ylabel('Standard Deviation')\n",
    "    ax1.set_yscale('log')\n",
    "    ax1.set_xticklabels(models)\n",
    "    ax1.set_title('Standard Deviations for HO (red) and CV (blue)')\n",
    "    ax1.yaxis.grid(True)\n",
    "    plt.legend(['HO', 'CV'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1) Hold-Out Method (4 Points)\n",
    "We will implement the hold-out method in this section. Please see below and fill out the missing code snippets. <br>\n",
    "Make sure that you follow the instructions written in the comments. Otherwise the code will not run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hold_out_method(data_in: np.ndarray, data_out: np.ndarray, split_coeff: float)->Tuple[dict, dict]:\n",
    "    \"\"\"\n",
    "    Splits the data into a training data set and a validation data set. \n",
    "    :param data_in: the input data which we want to split, shape: [n_data x indim_data] \n",
    "    :param data_out: the output data which we want to split, shape: [n_data x outdim_data]\n",
    "    Note: each data point i in data_in and data_out is considered as a training/validation sample -> (x_i, y_i)\n",
    "    :param split_coeff: a value between [0, 1], which determines the index to split data into test and validation set\n",
    "                        according to: split_idx = int(n_data*split_coeff)\n",
    "    :return: Returns a tuple of 2 dictionaries: the first element in the tuple is the training data set dictionary\n",
    "             containing the input data marked with key 'x' and the output data marked with key 'y'.\n",
    "             The second element in the tuple is the validation data set dictionary containing the input data \n",
    "             marked with key 'x' and the output data marked with key 'y'.\n",
    "    \"\"\"\n",
    "    n_data = data_in.shape[0]\n",
    "    # we use a dictionary to store the training and validation data. \n",
    "    # Please use 'x' as a key for the input data and 'y' as a key for the output data in the dictionaries\n",
    "    # for the training data and validation data\n",
    "    train_data = {}\n",
    "    val_data = {}\n",
    "    #---------------------------------------------------------------\n",
    "    # TODO\n",
    "    #---------------------------------------------------------------\n",
    "    return train_data, val_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Hold-Out (HO) on Regression\n",
    "This function will automatically call the functions you have implemented. It will make use of the Hold-out method\n",
    "on the 20 different data sets we have loaded at the beginning. It will return the standard deviation of the \n",
    "mean squarred errors of the 20 data sets of each different model it is tested on. You do not need to implement anything here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_ho_method(M: int, split_coeff: float, fit_func:callable, predict_func: callable):\n",
    "    \"\"\"\n",
    "    :param M: Model complexity param: for polynomial regression model order, for kNNR: number of neighbors\n",
    "    :param split_coeff:a value between [0, 1], which determines the index to split data into test and validation set\n",
    "                     according to: split_idx = int(n_data*split_coeff)\n",
    "    :param fit_func: callable function which will fit your model\n",
    "    :param predict_func: callable function which will make predictions with your model\n",
    "    \"\"\"\n",
    "    n_repetitions = 20 # we have 20 different data sets, we want to perform HO on....\n",
    "    models = np.arange(1, M+1)\n",
    "    mse_train_ho = np.zeros((n_repetitions, M))\n",
    "    mse_val_ho = np.zeros((n_repetitions, M))\n",
    "    \n",
    "    for rep in range(n_repetitions):\n",
    "        c_x_samples = x_samples[rep, :, :]    # extract the current data set \n",
    "        c_y_samples = y_samples[rep, :, :]    # extract the current data set\n",
    "        train_data, val_data = hold_out_method(c_x_samples, c_y_samples, split_coeff)\n",
    "        \n",
    "        for i, m in enumerate(models):\n",
    "            # 2: Train on training data to obtain \\hat{f}_{D_T}(x)\n",
    "            p = fit_func(train_data['x'], train_data['y'], m)\n",
    "            f_hat_D_T = predict_func(p, val_data['x'])\n",
    "            \n",
    "            # 3: Evaluate resulting estimators on validation data\n",
    "            mse_val_ho[rep, i] = np.mean((f_hat_D_T - val_data['y'])**2)\n",
    "            \n",
    "            # MSE on training set for comparison\n",
    "            y_train = predict_func(p, train_data['x'])\n",
    "            mse_train_ho[rep, i] = np.mean((y_train - train_data['y'])**2)\n",
    "            \n",
    "            # log parameters of best model order\n",
    "            if i == 0:\n",
    "                p_best_ho = p\n",
    "            else:\n",
    "                if mse_val_ho[rep, i] <= np.min(mse_val_ho[rep, :i].reshape(-1)):\n",
    "                    p_best_ho = p\n",
    "                    \n",
    "    # mean over all repetitions\n",
    "    mean_mse_train_ho = np.mean(mse_train_ho, axis=0)\n",
    "    mean_mse_val_ho = np.mean(mse_val_ho, axis=0)\n",
    "\n",
    "    std_mse_train_ho = np.std(mse_train_ho, axis=0)\n",
    "    std_mse_val_ho = np.std(mse_val_ho, axis=0)\n",
    "    \n",
    "    # 4: Pick model with best validation loss\n",
    "    m_star_idx_ho = np.argmin(mean_mse_val_ho) \n",
    "    m_star_ho = models[m_star_idx_ho]\n",
    "    print(\"Best model complexity for Hold out: {}\".format(m_star_ho))\n",
    "    \n",
    "    train_data, val_data = hold_out_method(x_samples[0, :, :], y_samples[0,:, :], split_coeff)\n",
    "    p_best_ho = fit_func(train_data['x'], train_data['y'], m_star_ho)\n",
    "    \n",
    "    # use only the first data set for better readability\n",
    "    plot(mse_val_ho, mse_train_ho, models, m_star_idx_ho, x_plt, y_plt,\n",
    "        x_samples[0, :, :], y_samples[0, :, :], p_best_ho, predict_func)\n",
    "    return std_mse_val_ho"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2) K-Fold-Cross Validation Method (4Points)\n",
    "We will implement the k-fold-cross validation method in this section. Please see below and fill out the missing code snippets. <br>\n",
    "Make sure that you follow the instructions written in the comments. Otherwise the code will not run. <br> <br>\n",
    "Please note that the function expects a callable 'fit' and a callable 'predict_func'. You will give different functions to this argument depending on the regression algorithm we consider. All of them have in common that the 'fit' function will return a model, whereas the 'predict_func' function will return a numpy array filled with predictions to the input data. <br>\n",
    "Please also have a look on the comments. You can also see in the function 'eval_cv_method' on how to use the \n",
    "'fit' and 'predict_func' functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cross_validation(data_in: np.ndarray, data_out: np.ndarray, m: int, k: int, fit: callable, \n",
    "                            predict_func: callable)->Tuple[np.ndarray, np.ndarray, float, float]:\n",
    "    \"\"\"\n",
    "    This function will split the data into a training set and a validation set and will shift the splitted data\n",
    "    k times and train k different models. It will return the mean squarred error of the training and the validation\n",
    "    data sets, based on the splits.\n",
    "    :param data_in: the input data which we want to split, shape: [N indim_data] \n",
    "    :param data_out: the output data which we want to split, shape: [N x outdim_data]\n",
    "    :param m: model parameter (e.g. polynomial degree, or number of nearest neighbors, ...). We will use this \n",
    "              variable to call the 'fit' function of your chosen model. Please see the function in the section\n",
    "              'Evaluation and Fit Functions'.\n",
    "              m is e.g. the degree of a polynomial\n",
    "              m is e.g. the parameter k for kNN Regression \n",
    "    :param k: number of partitions of the data set (not to be confused with k in kNN)\n",
    "    :param fit: callable function which will fit your model to the training data you provide -> expects \n",
    "                train_in (np.ndarray), train_out (np.ndarray), m (model parameter(s))  \n",
    "                (e.g. model order in polynomial regression), returns model params\n",
    "    :param predict_func: callable function which will use your model to do predictions on the input data \n",
    "                         you provide -> expects model params (-> m) and data_in (np.ndarray)\n",
    "    :return mse_train: np.ndarray containg the mean squarred errors for each training data in each split k shape [k]\n",
    "    :return mse_val: np.ndarray containing the mean squarred errors for the validation data in each split, shape[k]\n",
    "    \"\"\"\n",
    "    n_data = data_in.shape[0]\n",
    "    if k > n_data:\n",
    "        k = n_data\n",
    "    \n",
    "    # number of validation data\n",
    "    n_val_data = n_data//k   # e.g.: 15//2 = 7 \n",
    "    ind = np.arange(0, n_data)\n",
    "    mse_train = np.zeros(k)\n",
    "    mse_val = np.zeros(k)\n",
    "    \n",
    "    for i in range(k):\n",
    "        # 1: Split into 2 data sets\n",
    "        #---------------------------------------------------------------\n",
    "        # TODO\n",
    "        #---------------------------------------------------------------\n",
    "        \n",
    "        \n",
    "        # 2: get the training and validation data set\n",
    "        #---------------------------------------------------------------\n",
    "        # TODO\n",
    "        #---------------------------------------------------------------\n",
    "\n",
    "        \n",
    "        # 3: fit your model on training data\n",
    "        #Use here the 'fit' function. Expects (train_in:np.ndarray, train_out: np.ndarray, m)\n",
    "        \n",
    "        #---------------------------------------------------------------\n",
    "        # TODO\n",
    "        #---------------------------------------------------------------\n",
    "\n",
    "        \n",
    "        # 4: evaluate your model on training set and validation set\n",
    "        # Use here the 'predict_func' function. Expects (model you have fitted, data you want to make predictions).\n",
    "        \n",
    "        #---------------------------------------------------------------\n",
    "        # TODO\n",
    "        #---------------------------------------------------------------\n",
    "        \n",
    "        \n",
    "        # 5: assign performance: Calculate the mean squarred error for the training and validation set and\n",
    "        # write the result into the mse_train and mse_val arrays respectively\n",
    "        #---------------------------------------------------------------\n",
    "        # TODO\n",
    "        #---------------------------------------------------------------\n",
    "    return mse_train, mse_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate K-Fold-Cross Validation (CV) on Regression\n",
    "This function will automatically call the functions you have implemented. It will make use of the Cross Validation\n",
    "method on the 20 different data sets we have loaded at the beginning. It will return the standard deviation \n",
    "of the mean squarred errors of the 20 data sets of each different model it is tested on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_cv_method(M: int, k: int, fit_func:callable, predict_func: callable):\n",
    "    \"\"\"\n",
    "    :param M: Model complexity param: for polynomial regression model order, for kNNR: k number of neighbors\n",
    "    :param k: number of partitions\n",
    "    :param fit_func: callable function which will fit your model\n",
    "    :param predict_func: callable function which will use your model to perform predictions on data\n",
    "    \"\"\"\n",
    "    n_repetitions = 20 # we have 20 different data sets, we want to perform CV on....\n",
    "    models = np.arange(1, M+1) \n",
    "    \n",
    "    mse_train_cv = np.zeros((n_repetitions, M))\n",
    "    mse_val_cv = np.zeros((n_repetitions, M))\n",
    "    \n",
    "    for rep in range(n_repetitions):\n",
    "        c_x_samples = x_samples[rep, :, :]    # extract the current data set\n",
    "        c_y_samples = y_samples[rep, :, :]    # extract the current data set\n",
    "        \n",
    "        for i, m in enumerate(models):\n",
    "            mse_train, mse_val = k_fold_cross_validation(c_x_samples, c_y_samples, m, k, fit_func, predict_func)\n",
    "            mse_val_cv[rep, i] = np.mean(mse_val)\n",
    "            mse_train_cv[rep, i] = np.mean(mse_train)\n",
    "            \n",
    "    mean_mse_val_cv = np.mean(mse_val_cv, axis=0)\n",
    "    mean_mse_train_cv = np.mean(mse_train_cv, axis=0)\n",
    "    std_mse_val_cv = np.std(mse_val_cv, axis=0)    # calculates the standard deviation of the mse's over the 20 data sets\n",
    "    std_mse_train_cv = np.std(mse_train_cv, axis=0)# calculates the standard deviation of the mse's over the 20 data sets\n",
    "\n",
    "    m_star_idx_cv = np.argmin(mean_mse_val_cv)\n",
    "    m_star_cv = models[m_star_idx_cv]\n",
    "    print(\"Best model complexity for Cross Validation:\", m_star_cv)\n",
    "    \n",
    "    # use only the first data set for better readability\n",
    "    p_best_cv = fit_func(x_samples[0,: ,:], y_samples[0, :, :], m_star_cv)\n",
    "    \n",
    "    plot(mse_val_cv, mse_train_cv, models, m_star_idx_cv, x_plt, y_plt,\n",
    "        x_samples[0, :, :], y_samples[0, :, :], p_best_cv, predict_func)\n",
    "    \n",
    "    return std_mse_val_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3) KNN Regression\n",
    "We will apply Hold-out and K-Fold-Cross Validation on the regression problem using kNN Regression. In the following we provide a fit and a evaluate function for kNN, which will be directly used from the functions for hold-out and cross validation (as a callable). <br><br>\n",
    "Note that you will not be able to execute kNN Regression, if you haven't programmed the 'get_k_nearest' function from section 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_knn_regressor(train_in: np.ndarray, train_out:np.ndarray, k: int)->dict:\n",
    "    \"\"\"\n",
    "    This function will fit a knn model to the data. In fact, it will compactly represent the data provided.\n",
    "    I.e. it will store the training in- and output data together with the number of k neighbors in a dictionary.\n",
    "    :param train_in: the training input data, shape [N x input dim]\n",
    "    :param train_out: the training output data, shape [N x output dim]\n",
    "    :param k: the parameter how many nearest neighbors to choose.\n",
    "    :return: returns a dictionary containgin all the information:\n",
    "             The key 'x' marks the training input data (shape [N x input dim]).\n",
    "             The key 'y' marks the training output data (shape [N x output dimension]).\n",
    "             The key 'k' marks the parameter for k-nearest neighbors to be considered. \n",
    "    \"\"\"\n",
    "    model = {'x': train_in, 'y': train_out, 'k': k}\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_knn_regressor(model, data_in: np.ndarray)->np.ndarray:\n",
    "    \"\"\"\n",
    "    This function will perform predictions using a knn regression model given the input data. \n",
    "    Note that knn is a lazy model and requires to store\n",
    "    all the training data (see dictionary 'model').\n",
    "    :param model: dictionary containing the train data and the k parameter for k nearest neighbors. \n",
    "                  The key 'x' marks the training input data (shape [N x input dim]).\n",
    "                  The key 'y' marks the training output data (shape [N x output dimension]).\n",
    "                  The key 'k' marks the parameter for k-nearest neighbors to be considered.\n",
    "    :param data_in: the data we want to perform predictions (shape [N x input dimension])\n",
    "    :return prediction based on k nearest neighbors (mean of the k - neares neighbors) (shape[N x output dimension])\n",
    "    \"\"\"\n",
    "    if len(data_in.shape) == 1:\n",
    "        data_in = np.reshape(data_in, (-1, 1))\n",
    "    train_data_in = model['x']\n",
    "    train_data_out = model['y']\n",
    "    k = model['k']\n",
    "    if len(train_data_in) == 1:\n",
    "        train_data_in = np.reshape(train_data_in, (-1, 1))\n",
    "    predictions = np.zeros((data_in.shape[0], train_data_out.shape[1]))\n",
    "    for i in range(data_in.shape[0]):\n",
    "        c_data = data_in[i, :]\n",
    "        _, nearest_y = get_k_nearest(k, c_data, train_data_in, train_data_out)   \n",
    "        # we take the mean of the nearest samples to perform predictions\n",
    "        predictions[i, :] = np.mean(nearest_y, axis=0)    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1) Applying Hold-Out (HO) and Cross-Validation (CV)  to KNN Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply kNN Regression on our data set, where we vary the number of neighbors K (here denoted as variable M_knn). We apply Hold-out and Cross Validation on the 20 data sets and calculate the mean of the mean squarred error for each model, for both the Hold out method as well as the Cross Validation method. <br>\n",
    "We furthermore plot the standard deviation of the mean squarred errors for each model based on the 20 data sets. We compare the standard deviations resulting from the Hold out method and the Cross Validation method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_knn = 20     # Number of Neighbors K \n",
    "split_coeff = 0.8 # between 0,1: how many samples to split in Hold-out\n",
    "k = 15        # number of splits for Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hold Out\n",
    "std_mse_val_ho_knn = eval_ho_method(M=M_knn, split_coeff=split_coeff, fit_func=fit_knn_regressor, \n",
    "               predict_func=predict_knn_regressor)\n",
    "\n",
    "# Cross Validation\n",
    "std_mse_val_cv_knn = eval_cv_method(M=M_knn, k=k, fit_func=fit_knn_regressor, \n",
    "                                    predict_func=predict_knn_regressor)\n",
    "\n",
    "# Comparing the standard deviations\n",
    "plot_bars(M_knn, std_mse_val_ho_knn, std_mse_val_cv_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The first two rows in  the cell above showcase the errorplots and the best model's prediction for Hold out (first row) and Cross Validation(second row) respectively. <br>\n",
    "The last row in the above cell shows each model's standard deviation on the mean squarred error evaluated on the 20 different data sets. The red bars are the standard deviation for Hold-out and the blue bars are the standard deviations for Cross Validation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4) Forests\n",
    "We will apply Hold-out and K-Fold-Cross Validation on the regression problem using regression with forests. In the following we provide a fit and a evaluate function for forests, which will be directly used from the functions for hold-out and cross validation (as a callable). <br>\n",
    "Please note that we have two different functions for fitting a forest model. In 'fit_forest_fixed_n_trees' we investigate the behavior of the algorithm by fixing the number of trees to 1 and varying the number of sample per leaf. <br>\n",
    "In 'fit_forest_fixed_n_samples_leaf' we fix the number of samples per leaf to 1 and investigate the behavior of the algorithm by varying the number of trees. <br>\n",
    "The evaluation function can be used for both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_forest_fixed_n_trees(train_in: np.ndarray, train_out:np.ndarray, min_samples_leaf: int):\n",
    "    \"\"\"\n",
    "    This function will fit a forest model based on a fixed number of trees (can not be change when using this \n",
    "    function, is set globally)\n",
    "    :param train_in: the training input data, shape [N x input dim]\n",
    "    :param train_out: the training output data, shape [N x output dim]\n",
    "    :param min_samples_leaf: the number of samples per leaf to be used \n",
    "    \"\"\"\n",
    "    model = RandomForestRegressor(n_estimators=1, min_samples_leaf=min_samples_leaf)\n",
    "    model.fit(train_in, train_out)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_forest_fixed_n_samples_leaf(train_in: np.ndarray, train_out:np.ndarray, n_trees: int):\n",
    "    \"\"\"\n",
    "    This function will fit a forest model based on a fixed number of sample per leaf (can not be change when \n",
    "    using this function, is set globally)\n",
    "    :param train_in: the training input data, shape [N x input dim]\n",
    "    :param train_out: the training output data, shape [N x output dim]\n",
    "    :param n_trees: the number of trees in the forest \n",
    "    \"\"\"\n",
    "    model = RandomForestRegressor(n_estimators=n_trees, min_samples_leaf=1)\n",
    "    model.fit(train_in, train_out)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_forest(model, data_in: np.ndarray)->np.ndarray:\n",
    "    \"\"\"\n",
    "    This function will perform predictions using a forest regression model on the input data. \n",
    "    :param model: the forest model from scikit learn (fitted before)\n",
    "    :param data_in: :param data_in: the data we want to perform predictions (shape [N x input dimension])\n",
    "    :return prediction based on chosen minimum samples per leaf (shape[N x output dimension]\n",
    "    \"\"\"\n",
    "    y = model.predict(data_in)\n",
    "    if len(y.shape) == 1:\n",
    "        y = y.reshape((-1, 1))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.1) Applying Hold-Out (HO) and Cross-Validation (CV)  to Forests (Fixed number of Trees)\n",
    "\n",
    "We apply Forest Regression with fixed number of trees to 1 on our data set, where we vary the number of samples per leaf(here denoted as variable min_samples_leaf). We apply Hold-out and Cross Validation on the 20 data sets and calculate the mean of the mean squarred error for each model, for both the Hold out method as well as the Cross Validation method. <br>\n",
    "We furthermore plot the standard deviation of the mean squarred errors for each model based on the 20 data sets. We compare the standard deviations resulting from the Hold out method and the Cross Validation method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_samples_leaf = 10       # used when fixed number of trees and we want to evaluate number of samples per leaf\n",
    "split_coeff = 0.8           # between 0,1: how many samples to split in Hold-out\n",
    "k = 15                      # number of splits for Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hold-Out\n",
    "std_mse_val_ho_forest_fixed_n_trees = eval_ho_method(M=min_samples_leaf, split_coeff=split_coeff, \n",
    "                                                     fit_func=fit_forest_fixed_n_trees, \n",
    "                                                     predict_func=predict_forest)\n",
    "# Cross Validation \n",
    "std_mse_val_cv_forest_fixed_n_trees = eval_cv_method(M=min_samples_leaf, k=k, fit_func=fit_forest_fixed_n_trees, \n",
    "                                    predict_func=predict_forest)\n",
    "\n",
    "# Comparing the standard deviations\n",
    "plot_bars(min_samples_leaf, std_mse_val_ho_forest_fixed_n_trees, std_mse_val_cv_forest_fixed_n_trees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first two rows in  the cell above showcase the errorplots and the best model's prediction for Hold out (first row) and Cross Validation(second row) respectively. <br>\n",
    "The last row in the above cell shows each model's standard deviation on the mean squarred error evaluated on the 20 different data sets. The red bars are the standard deviation for Hold-out and the blue bars are the standard deviations for Cross Validation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2) Applying Hold-Out (HO) and Cross-Validation (CV)  to Forests (Fixed number of Samples per Leaf) \n",
    "We apply Forest Regression with fixed number of samples per leaf to 1 on our data set, where we vary the number of trees (here denoted as variable n_trees). We apply Hold-out and Cross Validation on the 20 data sets and calculate the mean of the mean squarred error for each model, for both the Hold out method as well as the Cross Validation method. <br>\n",
    "We furthermore plot the standard deviation of the mean squarred errors for each model based on the 20 data sets. We compare the standard deviations resulting from the Hold out method and the Cross Validation method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trees = 20       # used when fixed number of trees and we want to evaluate number of samples per leaf\n",
    "split_coeff = 0.8           # between 0,1: how many samples to split in Hold-out\n",
    "k = 15                      # number of splits for Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hold out\n",
    "std_mse_val_ho_forest_fixed_n_samples = eval_ho_method(M=n_trees, split_coeff=split_coeff, \n",
    "                                                       fit_func=fit_forest_fixed_n_samples_leaf,  \n",
    "                                                       predict_func=predict_forest)\n",
    "# Cross Validation\n",
    "std_mse_val_cv_forest_fixed_n_samples = eval_cv_method(M=n_trees, k=k, \n",
    "                                                       fit_func=fit_forest_fixed_n_samples_leaf, \n",
    "                                                       predict_func=predict_forest)\n",
    "\n",
    "\n",
    "# Comparing the standard deviations\n",
    "plot_bars(n_trees, std_mse_val_ho_forest_fixed_n_samples, std_mse_val_cv_forest_fixed_n_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The first two rows in  the cell above showcase the errorplots and the best model's prediction for Hold out (first row) and Cross Validation(second row) respectively. <br>\n",
    "The last row in the above cell shows each model's standard deviation on the mean squarred error evaluated on the 20 different data sets. The red bars are the standard deviation for Hold-out and the blue bars are the standard deviations for Cross Validation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5) Comparisons\n",
    "\n",
    "#### 3.5.1) (1 Point)\n",
    "Comparing the error plots from section 3.4.2) to the error plots from 3.3.1) and 3.4.1) we observe that the validation error does not increase with the number of trees. Give an intution for this observation. <br><br>\n",
    "\n",
    "\n",
    "\n",
    "#### 3.5.2) (1 Point)\n",
    "Compare the standard deviation plots from the last three sections. What is the main difference between Hold-out method and Cross Validation you can observe for all three Standard Deviation plots? Explain the reason for the observed behavior. <br> <br>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
